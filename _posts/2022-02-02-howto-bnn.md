---
title: "How to create a binary neural network - First results"
date: 2022-02-02T22:21:00.000Z
categories:
  - blog
tags:
  - geisten
  - tinyML
  - coregeist
published: false
---

After [describing the mathematics](../basic-math/) behind the binary neural network, this post is about the concrete implementation and shows the first results.

## Motivation

The trend towards deeper and broader networks has already reached a plateau reserved only for the big technology companies with the necessary capital and resources. Because of resource starvation, these models also require much more computing power and data storage or a permanent internet connection to exchange data with a powerful server. This limits the possible use cases and makes access difficult in areas with limited infrastructure.

The [coregeist](https://github.com/geisten/coregeist) project is our answer to these challenges. It shrinks existing neural models and adapts them better for systems with limited computing power. These adjusted models can then automatically generate C code optimized for the target system.

Pytorch and Tensorflow have been providing modules to compress deep learning models further so that they can run efficiently on smaller systems such as smartphones or microcontrollers. Typically, networks are trained with weight-matrices of 32bit or 16bit floating-point values. After training, the model is shrunk to 8 bits or even less by a post-process. Actual methods allow reducing the bit resolution to only some bits by adjusting the weights with discrete floating-point values. 
Going one step further, we reduce the resolution to just a single bit, with the goal of taking full advantage of the CPU's bit manipulation capabilities. We also completely abandon floating-point numbers when computing the neural network. This allows the efficient execution on even smaller controllers without FPU's (Floating-Point Processing Unit).

![](/assets/howto-bnn/bnn-process.png)

## Creating a simple neural network with PyTorch

We will first implement a simple network, including a hidden layer, with a 32-bit PyTorch model. The net is trained from scratch to measure the accuracy and learning duration. The gold standard for demonstrating neural networks is the MNIST database of handwritten digits. It contains 60000 training - and 10000 test data sets with 28x28 pixel images of handwritten numbers from 0-9. Our neural net will be built in three layers: Layer 1 with 748 neurons, Layer 2 with 120 neurons, and Layer 3 with 10 neurons.

In PyTorch, a neural network is built up out of modules. Modules can contain other modules, and a neural network is considered a module itself. We can now make use of the pre-defined modules in the torch.nn package, and define our own small neural network. We will use a minimal network with an input layer, one hidden layer with prelu as activation function, and an output layer:

```python
import torch


class SimpleClassifier(nn.Module):

  def __init__(self, downsample=None):
    super(SimpleClassifier, self).__init__()

    self.inputs = 28 * 28
    self.hidden = 120
    output = 10

    self.l1 = nn.Linear(self.inputs, self.hidden)
    self.action = nn.PReLU(self.hidden)
    self.l2 = nn.Linear(self.hidden, output)

  def forward(self, x):
    out = self.l1(x)
    out = self.action(out)
    out = self.l2(out)
    return out
```

By simply replacing the pytorch module `nn.Linear` with the new coregeist module `cg.Linear`, the model is converted to a binary training model.

```python
import coregeist as cg


class SimpleBNNClassifier(nn.Module):

  def __init__(self, downsample=None):
    super(SimpleClassifier, self).__init__()

    self.inputs = 28 * 28
    self.hidden = 120
    output = 10

    self.l1 = cg.Linear(self.inputs, self.hidden)
    self.action = nn.PReLU(self.hidden)
    self.l2 = cg.Linear(self.hidden, output)

  def forward(self, x):
    out = self.l1(x)
    out = self.action(out)
    out = self.l2(out)
    return out
```

The existing pytorch model can be optimized up to 98% accuracy after a few epochs (10).:

```shell
Test set: Avg. loss: 0.0763, Accuracy: 9805/10000 (98%)

total training time = 0.008005837268299527 hours
```

The training of the binary neural network looks different after 10 epochs:

```shell
Test set: Avg. loss: 1.1002, Accuracy: 8187/10000 (82%)

total training time = 0.015539689196480645 hours
```

Our binary neural net (BNN) is almost 16 percentage points worse and requires twice the training time. It became evident that a reduced resolution would not produce comparable results. However, the result is much better than expected with a 1-bit resolution. By increasing the training rounds (epochs), the accuracy can improve even further. We will improve the accuracy even more in the upcoming versions. However, my goal is not to achieve very high accuracy. Many applications already manage with lower precision. Even the 98% accuracy calculated with the 32-bit floating-point model is insufficient for many applications. Or would you get into a car with 98% accuracy in detecting dangers?

If we train the classical 32-bit floating-point model, the optimization proceeds continuously, increasing accuracy steadily:

![](/assets/howto-bnn/nn_train_vs_valid_losses.png)

![](/assets/howto-bnn/nn_train_vs_valid_accuracy.png)

However, if we look at the training history of the binary network, we notice that the accuracy jumps back and forth with each epoch. The discrete nature of binary networks makes it difficult to continuously improve the optimization.  

![](/assets/howto-bnn/bnn_train_vs_valid_losses.png)

![](/assets/howto-bnn/bnn_train_vs_valid_accuracy.png)

It would be interesting to see to what extent modifications of the parameters (hidden layers, etc.) influence the training.

This small experiment has shown that it is possible in principle to transform existing models into a binary neural network without significant changes. However, the current gradient optimization methods are not optimal for our unsteady binary network. The optimization process is slower with a higher error rate. It would be fascinating to investigate how other optimization methods perform. It was clear that a model with lower resolution would not deliver equivalent results. But, the result with a 1-bit resolution was better than expected in the first tests. By increasing the training rounds (epochs), the accuracy could be improved even further. We will try to increase the accuracy and focus on problems with the lower required accuracy. 

As a [next step](../export-bnn), we will convert the generated model into a C program to significantly speed up the neural net model by optimized bit operations on the target platform. 
